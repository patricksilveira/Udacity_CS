{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-6fc196761611>, line 126)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-6fc196761611>\"\u001b[0;36m, line \u001b[0;32m126\u001b[0m\n\u001b[0;31m    while:\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Crawler for weblinks\n",
    "\n",
    "# Get page input from user\n",
    "\n",
    "\n",
    "def record_user_click(index, keyboard, url):\n",
    "    urls = lookup(index, keyword) # check if we have  urls\n",
    "    if urls:\n",
    "        for entry in urls:\n",
    "            if entry[0] == url: # check if we have that entry in list index\n",
    "                entry[1] += entry[1] + 1 #add entry to count\n",
    "\n",
    "def get_all_links(page):\n",
    "    links = []\n",
    "    while True:\n",
    "        url, endpos = get_next_target(page)\n",
    "        if url:\n",
    "            links = links.append(url) # links.append(url) - links =\n",
    "            page = page[endpos:]\n",
    "        else:\n",
    "            break\n",
    "    return links\n",
    "\n",
    "\n",
    "def crawl_web(seed, max_pages):\n",
    "    tocrawl = [seed]\n",
    "    crawled = [] # len(crawled) in lenght of crawled\n",
    "    index={}\n",
    "    graph={}\n",
    "    while tocrawl:\n",
    "        page = tocrawl.pop()\n",
    "        # test if the page has already been crawled\n",
    "        if page not in crawled:\n",
    "            # use union to add pages, to avoid dupplication\n",
    "            content = get_page(page)\n",
    "            add_page_to_index(index,page,content)\n",
    "            outlinks = get_all_links(content)\n",
    "            graph[page] = outlinks\n",
    "            union(tocrawl, outlinks)\n",
    "            crawled.append(page)\n",
    "    return index, graph \n",
    "\n",
    "    \n",
    "def add_to_index(index,keyword,url):\n",
    "    if keyword in index:\n",
    "        index[keyword].append(url)\n",
    "    else:\n",
    "        index[keyword] = [url]\n",
    "\n",
    "# def add_to_index(index, keyword, url):\n",
    "#      format of index: [[keyword, [[url, count], [url, count],..]],...]\n",
    "#     for entry in index:\n",
    "#         if entry[0] == keyword: # check keyword\n",
    "#             for element in entry[1]: # loop [url,count] for duplicates\n",
    "#                 if element[0] == url: # check if [0] is our url\n",
    "#                     return\n",
    "#             entry[1].append([url,0]) # if not in list, append to it\n",
    "#             return\n",
    "#     not found, add new keyword to index\n",
    "#     index.append([keyword, [[url,0]]])\n",
    "\n",
    "\n",
    "\n",
    "def lookup(index, keyword):\n",
    "    if keyword in index:\n",
    "        return index[keyword]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "#def lookup(index,keyword):\n",
    "#for entry in index:\n",
    " #       if entry[0] == keyword:\n",
    "  #          result.append(entry[1])\n",
    "   # return result\n",
    "\n",
    "\n",
    "def get_page(page):\n",
    "    try:\n",
    "        import  urllib\n",
    "        return urllib.urlopen(url).read()\n",
    "    except:\n",
    "        return \"No page\"\n",
    "\n",
    "\n",
    "def get_next_target(page) :\n",
    "    start_link = page.find('<a href=')\n",
    "    if start_link == -1:\n",
    "        return None, 0\n",
    "    start_quote = page.find('\"',  start_link)\n",
    "    end_quote = page.find('\"', start_quote + 1)\n",
    "    url = page[start_quote + 1:end_quote]\n",
    "    return url, end_quote\n",
    "\n",
    "# Union of pages into crawler function\n",
    "\n",
    "def union(p,q):\n",
    "    for e in q:\n",
    "        if e not in p:\n",
    "            p.append(e)\n",
    "\n",
    "def add_page_to_index(index, url, content): \n",
    "    words = content.split() \n",
    "    for word in words: \n",
    "        add_to_index(index, word, url) \n",
    "        \n",
    "def print_all_links():\n",
    "    while:\n",
    "        url, endpos = get_next_target(page)\n",
    "        if url:\n",
    "            print(url)\n",
    "            page = page[endpos:]\n",
    "        else: \n",
    "            \n",
    "def compute_ranks(graph):\n",
    "    d = 0.8 # damping factor\n",
    "    numloops = 10 # number of times we go through relaxation\n",
    "    \n",
    "    ranks = {}\n",
    "    npages = len(graph)\n",
    "    for page in graph:\n",
    "        ranks[page] = 1.0 /npages\n",
    "\n",
    "    for i in range(0, numloops):\n",
    "        newranks = {}\n",
    "        for page in graph:\n",
    "            newrank = (1 - d) / npages \n",
    "            # update by summing in the inlink ranks\n",
    "            for node in graph:\n",
    "                if page in graph[node]:\n",
    "                    newrank = newrank + d * (ranks[nodes] / len(graph[node]))\n",
    "            newranks[page] = newrank \n",
    "        ranks = newranks\n",
    "    return ranks\n",
    "    \n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hashtable_add_teacher(htable,key,value):\n",
    "    hashtable_get_bucket(htable,key).append([key,value]) \n",
    "    \n",
    "def hashtable_get_bucket(htable,keyword):\n",
    "    return htable[hash_string(keyword, len(table))]\n",
    "\n",
    "\n",
    "def hash_string(keyword,buckets):\n",
    "    out = 0\n",
    "    for s in keyword:\n",
    "        out = (out + ord(s)) % buckets\n",
    "    return out\n",
    "\n",
    "def make_hashtable(nbuckets):\n",
    "    table = []\n",
    "    for unused in range(0,nbuckets):\n",
    "        table.append([])\n",
    "    return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtable_add(htable,key,value):\n",
    "    bucket = hash_string(key,len(htable))\n",
    "    htable[bucket].append([key,value])\n",
    "    return htable  \n",
    "\n",
    "def hashtable_add_teacher(htable,key,value):\n",
    "    hashtable_get_bucket(htable,key).append([key,value])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-0ee4a40cea17>, line 6)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-0ee4a40cea17>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    Return None\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def hashtable_lookup(htable, key):\n",
    "    bucket = hashtable_get_bucket(htable,key)\n",
    "    for entry in bucket:\n",
    "        if entry[0] == key:\n",
    "            return entry[1]\n",
    "    Return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtable_update(htable,key,value):\n",
    "    bucket = hashtable_get_bucket(htable,key)\n",
    "    for entry in bucket:\n",
    "        if entry[0] == key:\n",
    "            entry[1] = value\n",
    "            return\n",
    "    bucket.append([key,value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Relaxation Algorithm\n",
    "# Start with a guess\n",
    "#   while not done:\n",
    "#       make guess better \n",
    "# \n",
    "\n",
    "\n",
    "# #Rank Web Pages\n",
    "\n",
    "#rank(0,url) => 1\n",
    "#rank(t,url) => sum rank (t-1,p) p in inlink[url]\n",
    "# sum of all pages that has a link to that URL\n",
    "# t = timestamp == number of times we tried to guess\\\n",
    "# t-1 => try to find the popularity of the friend in the previous step\n",
    "\n",
    "def popularity(t,p):\n",
    "    if t == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        score = 0\n",
    "        for f in friends(p):\n",
    "            score = score + popularity(t-1,f)\n",
    "        return score\n",
    "\n",
    "\n",
    "#model a random web surfer\n",
    "#probability of reaching a certain page given a random starting point (page) \n",
    "# If we set a rank 0 for pages with no links, the surfer might enter (random) and it would make very difficult to access the page.popularity\n",
    "#when there are no incoming links we change from zero\n",
    "\n",
    "#we need to add a chance of getting to a 0 links page\n",
    "#dunpimg page => h\n",
    "\n",
    "d = damping constant = 0.8\n",
    "N = Number of Pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "crawl_web() missing 1 required positional argument: 'max_pages'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-beace62fb0e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m \u001b[0mindex\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrawl_web\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://udacity.com/cs101x/urank/index.html'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'http://udacity.com/cs101x/urank/index.html'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: crawl_web() missing 1 required positional argument: 'max_pages'"
     ]
    }
   ],
   "source": [
    "def crawl_web(seed, max_pages):\n",
    "    tocrawl = [seed]\n",
    "    crawled = [] # len(crawled) in lenght of crawled\n",
    "    index={}\n",
    "    graph={}\n",
    "    while tocrawl:\n",
    "        page = tocrawl.pop()\n",
    "        # test if the page has already been crawled\n",
    "        if page not in crawled:\n",
    "            # use union to add pages, to avoid dupplication\n",
    "            content = get_page(page)\n",
    "            add_page_to_index(index,page,content)\n",
    "            outlinks = get_all_links(content)\n",
    "            graph[page] = outlinks\n",
    "            union(tocrawl, outlinks)\n",
    "            crawled.append(page)\n",
    "    return index, graph \n",
    "\n",
    "\n",
    "\n",
    "cache = {\n",
    "   'http://udacity.com/cs101x/urank/index.html': \"\"\"<html>\n",
    "<body>\n",
    "<h1>Dave's Cooking Algorithms</h1>\n",
    "<p>\n",
    "Here are my favorite recipes:\n",
    "<ul>\n",
    "<li> <a href=\"http://udacity.com/cs101x/urank/hummus.html\">Hummus Recipe</a>\n",
    "<li> <a href=\"http://udacity.com/cs101x/urank/arsenic.html\">World's Best Hummus</a>\n",
    "<li> <a href=\"http://udacity.com/cs101x/urank/kathleen.html\">Kathleen's Hummus Recipe</a>\n",
    "</ul>\n",
    "\n",
    "For more expert opinions, check out the \n",
    "<a href=\"http://udacity.com/cs101x/urank/nickel.html\">Nickel Chef</a> \n",
    "and <a href=\"http://udacity.com/cs101x/urank/zinc.html\">Zinc Chef</a>.\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\", \n",
    "   'http://udacity.com/cs101x/urank/zinc.html': \"\"\"<html>\n",
    "<body>\n",
    "<h1>The Zinc Chef</h1>\n",
    "<p>\n",
    "I learned everything I know from \n",
    "<a href=\"http://udacity.com/cs101x/urank/nickel.html\">the Nickel Chef</a>.\n",
    "</p>\n",
    "<p>\n",
    "For great hummus, try \n",
    "<a href=\"http://udacity.com/cs101x/urank/arsenic.html\">this recipe</a>.\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\", \n",
    "   'http://udacity.com/cs101x/urank/nickel.html': \"\"\"<html>\n",
    "<body>\n",
    "<h1>The Nickel Chef</h1>\n",
    "<p>\n",
    "This is the\n",
    "<a href=\"http://udacity.com/cs101x/urank/kathleen.html\">\n",
    "best Hummus recipe!\n",
    "</a>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\", \n",
    "   'http://udacity.com/cs101x/urank/kathleen.html': \"\"\"<html>\n",
    "<body>\n",
    "<h1>\n",
    "Kathleen's Hummus Recipe\n",
    "</h1>\n",
    "<p>\n",
    "\n",
    "<ol>\n",
    "<li> Open a can of garbanzo beans.\n",
    "<li> Crush them in a blender.\n",
    "<li> Add 3 tablespoons of tahini sauce.\n",
    "<li> Squeeze in one lemon.\n",
    "<li> Add salt, pepper, and buttercream frosting to taste.\n",
    "</ol>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "\"\"\", \n",
    "   'http://udacity.com/cs101x/urank/arsenic.html': \"\"\"<html>\n",
    "<body>\n",
    "<h1>\n",
    "The Arsenic Chef's World Famous Hummus Recipe\n",
    "</h1>\n",
    "<p>\n",
    "\n",
    "<ol>\n",
    "<li> Kidnap the <a href=\"http://udacity.com/cs101x/urank/nickel.html\">Nickel Chef</a>.\n",
    "<li> Force her to make hummus for you.\n",
    "</ol>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "\"\"\", \n",
    "   'http://udacity.com/cs101x/urank/hummus.html': \"\"\"<html>\n",
    "<body>\n",
    "<h1>\n",
    "Hummus Recipe\n",
    "</h1>\n",
    "<p>\n",
    "\n",
    "<ol>\n",
    "<li> Go to the store and buy a container of hummus.\n",
    "<li> Open it.\n",
    "</ol>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\", \n",
    "}\n",
    "\n",
    "def get_page(url):\n",
    "    if url in cache:\n",
    "        return cache[url]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_next_target(page):\n",
    "    start_link = page.find('<a href=')\n",
    "    if start_link == -1: \n",
    "        return None, 0\n",
    "    start_quote = page.find('\"', start_link)\n",
    "    end_quote = page.find('\"', start_quote + 1)\n",
    "    url = page[start_quote + 1:end_quote]\n",
    "    return url, end_quote\n",
    "\n",
    "def get_all_links(page):\n",
    "    links = []\n",
    "    while True:\n",
    "        url, endpos = get_next_target(page)\n",
    "        if url:\n",
    "            links.append(url)\n",
    "            page = page[endpos:]\n",
    "        else:\n",
    "            break\n",
    "    return links\n",
    "\n",
    "\n",
    "def union(a, b):\n",
    "    for e in b:\n",
    "        if e not in a:\n",
    "            a.append(e)\n",
    "\n",
    "def add_page_to_index(index, url, content):\n",
    "    words = content.split()\n",
    "    for word in words:\n",
    "        add_to_index(index, word, url)\n",
    "        \n",
    "def add_to_index(index, keyword, url):\n",
    "    if keyword in index:\n",
    "        index[keyword].append(url)\n",
    "    else:\n",
    "        index[keyword] = [url]\n",
    "\n",
    "def lookup(index, keyword):\n",
    "    if keyword in index:\n",
    "        return index[keyword]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "index , graph = crawl_web('http://udacity.com/cs101x/urank/index.html') \n",
    "\n",
    "if 'http://udacity.com/cs101x/urank/index.html' in graph:\n",
    "    print(graph['http://udacity.com/cs101x/urank/index.html'])\n",
    "#>>> ['http://udacity.com/cs101x/urank/hummus.html',\n",
    "#'http://udacity.com/cs101x/urank/arsenic.html',\n",
    "#'http://udacity.com/cs101x/urank/kathleen.html',\n",
    "#'http://udacity.com/cs101x/urank/nickel.html',\n",
    "#'http://udacity.com/cs101x/urank/zinc.html']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}